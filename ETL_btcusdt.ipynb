{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/raghava/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 232, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/Users/raghava/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 191, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/Users/raghava/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 191, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"<ipython-input-2-cad7a68d56a6>\", line 50, in process_zip_with_chunks\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n  File \"/Users/raghava/anaconda3/lib/python3.7/zipfile.py\", line 1200, in __init__\n    self._RealGetContents()\n  File \"/Users/raghava/anaconda3/lib/python3.7/zipfile.py\", line 1267, in _RealGetContents\n    raise BadZipFile(\"File is not a zip file\")\nzipfile.BadZipFile: File is not a zip file\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cad7a68d56a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;31m# Read the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m \u001b[0mfinal_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_all_zips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;31m# data validations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cad7a68d56a6>\u001b[0m in \u001b[0;36mprocess_all_zips\u001b[0;34m(file_list)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_zip_with_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mfinal_pdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0mcareful\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0mto\u001b[0m \u001b[0myielded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \"\"\"\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "# Define the path where the zipped BTCUSDT data files are stored\n",
    "folder_path = \"./BTCUSDT\"\n",
    "\n",
    "# List all files in the specified folder and filter only the files\n",
    "file_list = os.listdir(folder_path)\n",
    "file_list = [os.path.join(folder_path, f) for f in file_list if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# Define column names and data types for reading CSV files\n",
    "column_names = [\n",
    "    'open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time',\n",
    "    'base_asset_volume', 'number_of_trades', 'taker_buy_volume', \n",
    "    'taker_buy_base_asset_volume', 'ignore'\n",
    "]\n",
    "\n",
    "\n",
    "# Define the expected data types for each column\n",
    "column_dtypes = {\n",
    "    'open_time': 'int64',\n",
    "    'open': 'float64',\n",
    "    'high': 'float64',\n",
    "    'low': 'float64',\n",
    "    'close': 'float64',\n",
    "    'volume': 'float64',\n",
    "    'close_time': 'int64',\n",
    "    'base_asset_volume': 'float64',\n",
    "    'number_of_trades': 'int64',\n",
    "    'taker_buy_volume': 'float64',\n",
    "    'taker_buy_base_asset_volume': 'float64',\n",
    "    'ignore': 'float64'\n",
    "}\n",
    "\n",
    "# Function to check for duplicates in specific columns of a DataFrame\n",
    "def dup_check(pdf: pd.DataFrame, dims: List[str]) -> None:\n",
    "    pdf_check = pdf[dims].drop_duplicates()\n",
    "    cond = len(pdf_check) == len(pdf)\n",
    "    msg = f\"Dups found in: {dims}\"\n",
    "    assert cond, msg\n",
    "\n",
    "# Function to validate if columns are of timestamp type and contain no null values\n",
    "def check_timetype(pdf: pd.DataFrame, dims: List[str]) -> None:\n",
    "    msg = f'One or more columns are not of timestamp type'\n",
    "    assert all(is_datetime64_any_dtype(pdf[c]) for c in dims), msg\n",
    "    \n",
    "    msg = f'One or more columns contain NaT values'\n",
    "    assert all(pdf[c].notna().all() for c in dims), msg\n",
    "       \n",
    "# Function to process a zip file in chunks and concatenate into a DataFrame\n",
    "def process_zip_with_chunks(zip_path:str, chunksize=50000)->pd.DataFrame:\n",
    "    pdfs = []\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        for file_name in zip_ref.namelist():\n",
    "            if file_name.endswith('.csv'):\n",
    "                with zip_ref.open(file_name) as file:\n",
    "                    chunk_iter = pd.read_csv(file, chunksize=chunksize,names=column_names, dtype=column_dtypes, header=None)\n",
    "                    for chunk in chunk_iter:\n",
    "                        pdfs.append(chunk)\n",
    "                        \n",
    "    return pd.concat(pdfs, ignore_index=True)\n",
    "\n",
    "# Function to process all zip files using parallel processing\n",
    "def process_all_zips(file_list:List[str])->pd.DataFrame:\n",
    "    final_pdfs = []\n",
    "    \n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = executor.map(process_zip_with_chunks, file_list)\n",
    "        \n",
    "        for pdf in results:\n",
    "            final_pdfs.append(pdf)\n",
    "    \n",
    "    final_pdf = pd.concat(final_pdfs, ignore_index=True)\n",
    "    return final_pdf\n",
    "\n",
    "# Function to check for time completeness and identify any missing time intervals\n",
    "def check_time_completeness(final_clean_pdf: pd.DataFrame) -> Tuple[List[str], List[str]]:    \n",
    "    # Calculate the min and max time for open_time and close_time\n",
    "    first_open_time = final_clean_pdf['open_time'].min()\n",
    "    last_open_time = final_clean_pdf['open_time'].max()\n",
    "    first_close_time = final_clean_pdf['close_time'].min()\n",
    "    last_close_time = final_clean_pdf['close_time'].max()\n",
    "\n",
    "    print(f\"First open time in dataset: {first_open_time}\")\n",
    "    print(f\"Last open time in dataset: {last_open_time}\")\n",
    "    print(f\"First close time in dataset: {first_close_time}\")\n",
    "    print(f\"Last close time in dataset: {last_close_time}\")\n",
    "\n",
    "    # Define the expected time range\n",
    "    expected_start = pd.Timestamp(\"2017-08-01 04:00:00\")\n",
    "    expected_end = pd.Timestamp(\"2024-06-30 23:59:00\")\n",
    "    \n",
    "    expected_close_start = pd.Timestamp(\"2017-08-01 04:00:59.999\")\n",
    "    expected_close_end = pd.Timestamp(\"2024-06-30 23:59:59.999\")\n",
    "    \n",
    "    # Check if the time range matches the expected range\n",
    "    if first_open_time >= expected_start and last_open_time <= expected_end:\n",
    "        print(\"The data covers the expected time range.\")\n",
    "    else:\n",
    "        print(\"The data does not cover the expected open time range. Check for missing or excess data.\")\n",
    "        \n",
    "    if first_close_time >= expected_close_start and last_close_time <= expected_close_end:\n",
    "        print(\"The data covers the expected time range.\")\n",
    "    else:\n",
    "        print(\"The data does not cover the expected close time range. Check for missing or excess data.\")\n",
    "\n",
    "    # Generate the full date range with 1-minute intervals\n",
    "    full_open_time_range = pd.date_range(start=first_open_time, end=last_open_time, freq='1T')\n",
    "    full_close_time_range = pd.date_range(start=first_close_time, end=last_close_time, freq='1T')\n",
    "\n",
    "    # Extract actual open_time and close_time values from the dataset\n",
    "    actual_open_times = final_clean_pdf['open_time']\n",
    "    actual_close_times = final_clean_pdf['close_time']\n",
    "\n",
    "    # Compare the two time ranges to find missing timestamps\n",
    "    missing_open_times = full_open_time_range.difference(actual_open_times)\n",
    "    missing_close_times = full_close_time_range.difference(actual_close_times)\n",
    "    \n",
    "    # Output the results    \n",
    "    if len(missing_open_times) == 0:\n",
    "        print(\"No missing 1-minute intervals in the dataset for open_time.\")\n",
    "    else:\n",
    "        print(f\"Missing 1-minute intervals found for open_time: {len(missing_open_times)}\")\n",
    "        missing_open_times_list=missing_open_times.strftime(\"%Y-%m-%d %H:%M:%S\").tolist()\n",
    "        \n",
    "    if len(missing_close_times) == 0:\n",
    "        print(\"No missing 1-minute intervals in the dataset for close_time.\")\n",
    "    else:\n",
    "        print(f\"Missing 1-minute intervals found for close_time: {len(missing_close_times)}\")\n",
    "        missing_close_times_list=missing_close_times.strftime(\"%Y-%m-%d %H:%M:%S\").tolist()\n",
    "    \n",
    "    return missing_open_times_list, missing_close_times_list\n",
    "\n",
    "# Function to identify and capture rejected records based on data quality checks\n",
    "def capture_rejected_records(final_clean_pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Filter records where 'high' value is greater than 'low' value\n",
    "    non_rejected_record = final_clean_pdf[final_clean_pdf['high'] > final_clean_pdf['low']]\n",
    "    rejected_record = final_clean_pdf[final_clean_pdf['high'] < final_clean_pdf['low']]\n",
    "    \n",
    "    # Ensure that no incorrect records are present\n",
    "    cond = len(rejected_record) == 0\n",
    "    msg = f\"High value is greater than low which is not correct\"\n",
    "    assert cond, msg\n",
    "\n",
    "    # Columns to check for null values\n",
    "    columns_to_check = ['open_time', 'open', 'high', 'low', 'close', 'volume', \n",
    "                        'close_time', 'base_asset_volume', 'number_of_trades', \n",
    "                        'taker_buy_volume', 'taker_buy_base_asset_volume']\n",
    "\n",
    "    # Identify records with null values\n",
    "    mask = non_rejected_record[columns_to_check].isnull().any(axis=1)\n",
    "    records_with_nulls = non_rejected_record[mask]\n",
    "    \n",
    "    # Create a DataFrame of rejected records\n",
    "    rejected_pdf = pd.concat([rejected_record, records_with_nulls], ignore_index=True)\n",
    "\n",
    "    mask = non_rejected_record[columns_to_check].notnull().any(axis=1)\n",
    "\n",
    "    # Capture the rows with non null values\n",
    "    non_rejected_pdf = non_rejected_record[mask]\n",
    "\n",
    "    cond = len(rejected_record) == 0\n",
    "    msg = f\"High value is greater than low which is not correct\"\n",
    "    assert cond, msg\n",
    "    \n",
    "    return non_rejected_record, rejected_pdf\n",
    "        \n",
    "# Function to plot volume trends with a 7-day rolling average\n",
    "def plot_volume_trends(non_rejected_record:pd.DataFrame):\n",
    "    \n",
    "    %matplotlib inline\n",
    "    \n",
    "    # Calculate the 7-day rolling average of volume\n",
    "    non_rejected_record['volume_rolling_7d'] = non_rejected_record['volume'].rolling(window=7).mean()\n",
    "\n",
    "    # Plot volume and its rolling average over time\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(non_rejected_record['open_time'], non_rejected_record['volume'], label='Volume', color='orange')\n",
    "    plt.plot(non_rejected_record['open_time'], non_rejected_record['volume_rolling_7d'], label='7-day Rolling Avg Volume', color='blue')\n",
    "\n",
    "    plt.title('BTCUSDT Volume Trends Over Time with 7-day Rolling Average')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Volume')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "# Function to extract additional features for further analysis\n",
    "def feature_extract(non_rejected_pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # Calculate price and volume changes\n",
    "    non_rejected_pdf['open_close_price_change'] = non_rejected_pdf['close'] - non_rejected_pdf['open']\n",
    "    non_rejected_pdf['high_low_price_change'] = non_rejected_pdf['high'] - non_rejected_pdf['low']\n",
    "    non_rejected_pdf['open_prev_close_change'] = (non_rejected_pdf['open'] - non_rejected_pdf['close'].shift(-1))\n",
    "    non_rejected_pdf['close_next_open_change'] = (non_rejected_pdf['close'] - non_rejected_pdf['open'].shift(1))\n",
    "    non_rejected_pdf['vol_change'] = (non_rejected_pdf['volume'] - non_rejected_pdf['volume'].shift(-1))\n",
    "    \n",
    "    # Replacing null caused by shift operation with 0\n",
    "    non_rejected_pdf['open_prev_close_change']=non_rejected_pdf['open_prev_close_change'].fillna(0)\n",
    "    non_rejected_pdf['close_next_open_change']=non_rejected_pdf['close_next_open_change'].fillna(0)\n",
    "    non_rejected_pdf['vol_change']=non_rejected_pdf['vol_change'].fillna(0)\n",
    "\n",
    "    return non_rejected_pdf\n",
    "\n",
    "# Normalize the data based on Z-score calculation for existing and extracted features.\n",
    "def normalize_feature(non_rejected_pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # Calculate Z-scores for existing features (open, close, high, low, and volume)\n",
    "    non_rejected_pdf[\"z_score_open\"] = (non_rejected_pdf['open'] - non_rejected_pdf['open'].mean())/(non_rejected_pdf['open'].std())\n",
    "    non_rejected_pdf[\"z_score_close\"] = (non_rejected_pdf['close'] - non_rejected_pdf['close'].mean())/(non_rejected_pdf['close'].std())\n",
    "    non_rejected_pdf[\"z_score_high\"] = (non_rejected_pdf['high'] - non_rejected_pdf['high'].mean())/(non_rejected_pdf['high'].std())\n",
    "    non_rejected_pdf[\"z_score_low\"] = (non_rejected_pdf['low'] - non_rejected_pdf['low'].mean())/(non_rejected_pdf['low'].std())\n",
    "    non_rejected_pdf[\"z_score_vol\"] = (non_rejected_pdf['volume'] - non_rejected_pdf['volume'].mean())/(non_rejected_pdf['volume'].std())\n",
    "\n",
    "    # Calculate Z-scores for extracted features (volume change and price changes)\n",
    "    non_rejected_pdf[\"z_score_vol_change\"] = (non_rejected_pdf['vol_change'] - non_rejected_pdf['vol_change'].mean())/(non_rejected_pdf['vol_change'].std())\n",
    "    non_rejected_pdf[\"z_score_close_next_open_change\"] = (non_rejected_pdf['close_next_open_change'] - non_rejected_pdf['close_next_open_change'].mean())/(non_rejected_pdf['close_next_open_change'].std())\n",
    "    non_rejected_pdf[\"z_score_open_prev_close_change\"] = (non_rejected_pdf['open_prev_close_change'] - non_rejected_pdf['open_prev_close_change'].mean())/(non_rejected_pdf['open_prev_close_change'].std())\n",
    "    non_rejected_pdf[\"z_score_open_close_price_change\"] = (non_rejected_pdf['open_close_price_change'] - non_rejected_pdf['open_close_price_change'].mean())/(non_rejected_pdf['open_close_price_change'].std())\n",
    "    non_rejected_pdf[\"Z_score_high_low_price_change\"] = (non_rejected_pdf['high_low_price_change'] - non_rejected_pdf['high_low_price_change'].mean())/(non_rejected_pdf['high_low_price_change'].std())\n",
    "\n",
    "    # Return the DataFrame with normalized features\n",
    "    return non_rejected_pdf\n",
    "\n",
    "# Function to plot Z-score distribution of volume to identify outlier\n",
    "def capture_univariate_anamolies(non_rejected_pdf: pd.DataFrame) -> None:\n",
    "    # Define the columns for existing feature Z-scores\n",
    "    existing_feature_cols = [\"z_score_open\",\"z_score_close\",\"z_score_high\",\"z_score_low\",\"z_score_vol\"]\n",
    "    \n",
    "    # Loop through existing feature Z-scores and identify anomalies\n",
    "    for col in existing_feature_cols:\n",
    "        # Filter rows where the Z-score indicates an outlier (>3 or <-3)\n",
    "        existing_feature_anomolies= non_rejected_pdf[(non_rejected_pdf[col] > 3) | (non_rejected_pdf[col] < -3)]\n",
    "        # Calculate and print the percentage of anomalies for the current feature\n",
    "        print(f\"Univariate anamoly of existing feature:{len(existing_feature_anomolies)/len(non_rejected_pdf) * 100}\")\n",
    "        # Plot the Z-score anomalies for the current feature\n",
    "        plot_z_score_anomalies(non_rejected_pdf, col)\n",
    "\n",
    "    # Define the columns for extracted feature Z-scores\n",
    "    extracted_feature_cols = [\"z_score_vol_change\",\"z_score_close_next_open_change\",\"z_score_open_prev_close_change\",\"z_score_open_close_price_change\",\"Z_score_high_low_price_change\"]\n",
    "    \n",
    "    # Loop through existing feature Z-scores and identify anomalies\n",
    "    for col in extracted_feature_cols:\n",
    "        # Filter rows where the Z-score indicates an outlier (>3 or <-3)\n",
    "        extracted_feature_anomolies= non_rejected_pdf[(non_rejected_pdf[col] > 3) | (non_rejected_pdf[col] < -3)]\n",
    "        # Calculate and print the percentage of anomalies for the current feature\n",
    "        print(f\"Univariate anamoly of extracted feature:{len(extracted_feature_anomolies)/len(non_rejected_pdf) * 100}\")\n",
    "        # Plot the Z-score anomalies for the current feature\n",
    "        plot_z_score_anomalies(non_rejected_pdf, col)\n",
    "        \n",
    "    # Generate boxplots for visual comparison of Z-scores in existing and extracted features\n",
    "    gen_box_plot(non_rejected_pdf, existing_feature_cols)\n",
    "    gen_box_plot(non_rejected_pdf, extracted_feature_cols)\n",
    "    \n",
    "# Function to generate boxplots for multiple features in a DataFrame\n",
    "def gen_box_plot(non_rejected_pdf: pd.DataFrame, cols:List[str])->None:\n",
    "    \n",
    "    %matplotlib inline\n",
    "    # Create a figure for the boxplot with specified size\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Generate a boxplot for the selected columns\n",
    "    sns.boxplot(data=non_rejected_pdf[cols])\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(\"Boxplot of BTCUSDT Data\")\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "#Plots the Z-score of a specified feature and highlights data points that are classified as anomalies (Z-score > 3 or < -3).\n",
    "def plot_z_score_anomalies(non_rejected_pdf: pd.DataFrame, feature:str)->None:\n",
    "    \n",
    "    %matplotlib inline\n",
    "    \n",
    "    # Identify anomalies where Z-score is greater than 3 or less than -3\n",
    "    anomalies = non_rejected_pdf[(non_rejected_pdf[f'{feature}'] > 3) | (non_rejected_pdf[f'{feature}'] < -3)]\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot the Z-score values over time\n",
    "    plt.plot(non_rejected_pdf['open_time'], non_rejected_pdf[feature], label=f'{feature.capitalize()}')\n",
    "    \n",
    "    # Highlight anomalies with red 'x' markers\n",
    "    plt.scatter(anomalies['open_time'], anomalies[feature], color='red', label='Anomalies', marker='x')\n",
    "\n",
    "    # Add title and labels to the plot\n",
    "    plt.title(f'{feature.capitalize()} with Anomalies Highlighted')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(feature.capitalize())\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "#Calculates the Mahalanobis distance for each observation in the dataset to identify multivariate anomalies based on the distance from the mean considering the covariance of the data.\n",
    "def multivariate_anamoly_mahalanobis(selected_data)->pd.DataFrame:\n",
    "\n",
    "    # Calculate the mean vector and covariance matrix of the data\n",
    "    mean_vector = selected_data.mean(axis=0)\n",
    "    cov_matrix = np.cov(selected_data.T)\n",
    "\n",
    "    # Invert the covariance matrix\n",
    "    cov_matrix_inv = np.linalg.inv(cov_matrix)\n",
    "\n",
    "    # Calculate Mahalanobis distance for each row\n",
    "    mahalanobis_distances = selected_data.apply(lambda row: mahalanobis(row, mean_vector, cov_matrix_inv), axis=1)\n",
    "\n",
    "    # Set a threshold for identifying outliers (95th percentile of the distances)\n",
    "    threshold = np.percentile(mahalanobis_distances, 95)\n",
    "\n",
    "    # Flag outliers based on the threshold\n",
    "    outliers_mahalanobis = selected_data[mahalanobis_distances > threshold]\n",
    "\n",
    "    # Display the outliers\n",
    "    print(f\"Multivariate anamoly calculated using Mahalanobis for {selected_data.columns} feature:{len(outliers_mahalanobis)/len(selected_data) * 100}\")\n",
    "    \n",
    "    return outliers_mahalanobis\n",
    "\n",
    "\n",
    "\n",
    "# Read the data        \n",
    "final_pdf = process_all_zips(file_list)\n",
    "\n",
    "# data validations\n",
    "# converting time in int64 to datetime64 and validating it\n",
    "final_pdf['open_time'] = pd.to_datetime(final_pdf['open_time'], unit='ms')\n",
    "final_pdf['close_time'] = pd.to_datetime(final_pdf['close_time'], unit='ms')\n",
    "\n",
    "# default value to close time if its null\n",
    "null_closetime = final_pdf[final_pdf['close_time'].isna()]\n",
    "non_null_closetime = final_pdf[final_pdf['close_time'].notna()]\n",
    "null_closetime['close_time'] = null_closetime['open_time'] + pd.Timedelta(seconds=59.999)\n",
    "final_clean_pdf = pd.concat([null_closetime, non_null_closetime], ignore_index=True)\n",
    "\n",
    "check_timetype(final_clean_pdf,['open_time','close_time'])\n",
    "\n",
    "## Data Quality Check\n",
    "# check dups in close and open time\n",
    "dup_check(final_clean_pdf, ['open_time'])\n",
    "dup_check(final_clean_pdf, ['close_time'])\n",
    "\n",
    "# checking if both close and open time as bitcoin exchange opens for 24/7 including weekend\n",
    "missing_open_times_list, missing_close_times_list = check_time_completeness(final_clean_pdf)\n",
    "\n",
    "# Remove wrong records with anamolies \n",
    "non_rejected_pdf, rejected_pdf = capture_rejected_records(final_clean_pdf) \n",
    "\n",
    "# volume trend analysis\n",
    "plot_volume_trends(non_rejected_pdf)\n",
    "\n",
    "# extracting new features based on existing feature\n",
    "non_rejected_pdf=feature_extract(non_rejected_pdf)\n",
    "\n",
    "# calculate z_score for both existing and extracted feature\n",
    "non_rejected_pdf = normalize_feature(non_rejected_pdf)\n",
    "\n",
    "# univariate anamoly detection using box plot and z-score\n",
    "capture_univariate_anamolies(non_rejected_pdf)\n",
    "\n",
    "#multivariate anamoly detection using Mahalanobis distance\n",
    "#value change of existing feature\n",
    "existing_feature_cols = [\"z_score_open\",\"z_score_close\",\"z_score_high\",\"z_score_low\",\"z_score_vol\"]\n",
    "non_rejected_pdf_existing=non_rejected_pdf[existing_feature_cols]\n",
    "existing_feature_outlier=multivariate_anamoly_mahalanobis(selected_data=non_rejected_pdf_existing)\n",
    "\n",
    "#value change of extracted feature\n",
    "non_rejected_pdf_extracted_value=non_rejected_pdf[['z_score_vol_change','z_score_close_next_open_change','z_score_open_prev_close_change','z_score_open_close_price_change','Z_score_high_low_price_change']]\n",
    "extracted_feature_outlier_value=multivariate_anamoly_mahalanobis(selected_data=non_rejected_pdf_extracted_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
